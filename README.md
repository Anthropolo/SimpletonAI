# Simpleton AI Framework

A NextJS-based backend framework for AI tasks with built-in CLI, Ollama integration, and vector store capabilities. This framework provides a seamless way to process datasets, create embeddings, and serve AI models through a modern web interface.

## Features

- ğŸš€ Built on Next.js and TypeScript
- ğŸ› ï¸ Powerful CLI interface
- ğŸ“Š Support for CSV and text dataset processing
- ğŸ§  Integrated with Ollama for embeddings and inference
- ğŸ’¾ In-memory vector store with persistence
- ğŸ”„ Automatic data chunking and processing
- ğŸŒ RESTful API endpoints
- âš¡ Real-time inference capabilities
- ğŸ¨ Extensible React components
- ğŸ“ Local file storage system
- ğŸ¤– Ready-to-use AI application templates
- ğŸ” Built-in RAG (Retrieval-Augmented Generation) support
- ğŸ’¬ Chatbot creation capabilities
- ğŸ¯ AI Agent framework

## Prerequisites

- Node.js 18+ and npm
- Ollama installed locally (for embeddings and inference)
- TypeScript knowledge
- 2GB+ RAM recommended for vector operations

## Quick Start

1. Install SimpletonAI:
```bash
npm install simpleton-ai
```

2. Start Ollama (required for embeddings and inference):
```bash
ollama serve
```

3. Create a new Next.js project with SimpletonAI:
```bash
npx create-next-app@latest my-ai-app --typescript --tailwind
cd my-ai-app
```

4. Add SimpletonAI to your project:
```typescript
// pages/api/chat.ts
import { createChatbot } from 'simpleton-ai';

export default async function handler(req, res) {
  const chatbot = await createChatbot();
  const response = await chatbot.chat(req.body.message);
  res.json({ response });
}
```

5. Start the development server:
```bash
npm run dev
```

Visit `http://localhost:3000` to see your AI application in action!

## CLI Usage

After installing SimpletonAI, you can use the CLI globally:
```bash
npx simpleton-ai <command>
```

Or add it to your package.json scripts:
```json
{
  "scripts": {
    "process-data": "simpleton-ai upload ./data/knowledge_base.csv",
    "create-vectors": "simpleton-ai vectorize"
  }
}
```

## Use Cases

### 1. Building a RAG Application

```typescript
import { createRagApplication } from 'simpleton-ai';

async function main() {
  const rag = await createRagApplication();
  
  // Query your knowledge base
  const answer = await rag.queryKnowledgeBase(
    "What are the key features of SimpletonAI?"
  );
}
```

### 2. Creating a Chatbot

```typescript
import { createChatbot } from 'simpleton-ai';

async function main() {
  const chatbot = await createChatbot();
  
  // Start chatting
  const response = await chatbot.chat("Tell me about AI frameworks");
  
  // Get conversation history
  const history = chatbot.getHistory();
}
```

### 3. Building an AI Agent

```typescript
import { createAgent } from 'simpleton-ai';

async function main() {
  const agent = await createAgent();
  
  // Add custom tools
  agent.addTool({
    name: 'weather',
    description: 'Get weather information',
    execute: async (location) => {
      // Implement weather lookup
    }
  });
  
  // Let the agent solve tasks
  const result = await agent.think(
    "Analyze the weather data and summarize the trends"
  );
}
```

## API Reference

### RAG Endpoints

#### POST /api/rag/query
Query your knowledge base with RAG.

Request:
```json
{
  "query": "What is SimpletonAI?",
  "options": {
    "numResults": 3,
    "threshold": 0.7
  }
}
```

### Chatbot Endpoints

#### POST /api/chat
Interact with the chatbot.

Request:
```json
{
  "message": "Tell me about AI",
  "conversationId": "123",
  "options": {
    "temperature": 0.7
  }
}
```

### Agent Endpoints

#### POST /api/agent/task
Submit a task to the AI agent.

Request:
```json
{
  "task": "Analyze this dataset",
  "tools": ["search", "calculate"],
  "context": {
    "datasetId": "123"
  }
}
```

## Architecture

### Directory Structure
```
.
â”œâ”€â”€ data/                  # Data storage
â”‚   â”œâ”€â”€ uploads/          # Raw dataset files
â”‚   â”œâ”€â”€ chunks/           # Processed data chunks
â”‚   â”œâ”€â”€ vectors/          # Vector embeddings
â”‚   â””â”€â”€ models/           # Trained models
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ cli/             # CLI implementation
â”‚   â”œâ”€â”€ lib/             # Core libraries
â”‚   â”‚   â”œâ”€â”€ vectorStore.ts
â”‚   â”‚   â”œâ”€â”€ ollamaClient.ts
â”‚   â”‚   â”œâ”€â”€ rag.ts
â”‚   â”‚   â”œâ”€â”€ chatbot.ts
â”‚   â”‚   â””â”€â”€ agent.ts
â”‚   â”œâ”€â”€ app/             # Next.js app
â”‚   â””â”€â”€ components/      # React components
â””â”€â”€ examples/            # Example implementations
    â”œâ”€â”€ rag.ts
    â”œâ”€â”€ chatbot.ts
    â”œâ”€â”€ agent.ts
    â””â”€â”€ integration.ts
```

## Best Practices

### 1. Data Management
- Organize data into logical datasets
- Use appropriate chunk sizes (default 1000 tokens)
- Regularly update vector embeddings
- Monitor vector store size

### 2. Performance
- Implement caching for frequent queries
- Use batch processing for large datasets
- Configure vector store parameters based on your data size

### 3. Security
- Secure your Ollama endpoint
- Implement rate limiting
- Validate all user inputs
- Handle sensitive information properly

### 4. Development
- Start with example templates
- Use TypeScript for type safety
- Follow the modular architecture
- Write tests for custom tools

## Configuration

```json
{
  "vectorStore": {
    "dimension": 768,
    "similarity": "cosine",
    "maxElements": 100000
  },
  "ollama": {
    "baseUrl": "http://127.0.0.1:11434",
    "defaultModel": "nomic-embed-text",
    "timeout": 30000
  },
  "rag": {
    "numResults": 3,
    "minSimilarity": 0.7
  }
}
```

## Contributing

We welcome contributions! Please see our [Contributing Guide](CONTRIBUTING.md) for details.

## License

MIT License - see [LICENSE](LICENSE) for details.
